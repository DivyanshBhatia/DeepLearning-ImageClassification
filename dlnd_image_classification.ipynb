{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b915477f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (x-x.min(axis=0))/(x.max(axis=0)-x.min(axis=0))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb=None\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    global lb\n",
    "    if lb is None:\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "    encoding = lb.fit(np.array([[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0]]))\n",
    "    return encoding.transform(x)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,shape=[None,image_shape[0],image_shape[1],image_shape[2]],name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.int32,shape=[None,n_classes],name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    F_W=tf.Variable(tf.truncated_normal([*conv_ksize, int(x_tensor.get_shape()[3]), conv_num_outputs], dtype=tf.float32))\n",
    "    F_B = tf.Variable(tf.zeros(conv_num_outputs)) \n",
    "    strides = [1, conv_strides[0],conv_strides[1], 1]\n",
    "    \n",
    "    padding='SAME'\n",
    "    conv1 = tf.nn.conv2d(x_tensor,F_W, strides, padding) + F_B\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "    conv1,\n",
    "    ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "    strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "    padding=padding)\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    old_shape = x_tensor.get_shape()\n",
    "    flattened_shape = np.prod([x for x in old_shape[1:]])\n",
    "    new_shape = [-1,flattened_shape.value]\n",
    "    return tf.reshape(x_tensor, new_shape)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #Implementation: WX+B and apply Relu on it\n",
    "    FC_W=tf.Variable(tf.truncated_normal([int(x_tensor.get_shape()[1]),num_outputs], mean=0.0, stddev=0.1))\n",
    "    FC_B=tf.Variable(tf.zeros(num_outputs))\n",
    "    #WX\n",
    "    FC_WX=tf.matmul(tf.to_float(x_tensor),FC_W)\n",
    "    #WX+B\n",
    "    FC_WXB=tf.add(FC_WX,FC_B)\n",
    "    \n",
    "    #Applying Relu for negative weights\n",
    "    FC_Relu=tf.nn.relu(FC_WXB)\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    return FC_Relu\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #Implementation: WX+B and apply Relu on it\n",
    "    O_W=tf.Variable(tf.truncated_normal([int(x_tensor.get_shape()[1]),num_outputs], mean=0.0, stddev=0.1))\n",
    "    O_B=tf.Variable(tf.zeros(num_outputs))\n",
    "    #WX\n",
    "    O_WX=tf.matmul(x_tensor,O_W)\n",
    "    #WX+B\n",
    "    O_WXB=tf.add(O_WX,O_B)\n",
    "    return O_WXB\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_tensor=x\n",
    "    conv_num_outputs=10\n",
    "    conv_p1_outputs=32\n",
    "    conv_p2_outputs=64\n",
    "    conv_p3_outputs=128\n",
    "    conv_ksize=(2,2)\n",
    "    conv_strides=(2,2)\n",
    "    pool_ksize=(2,2)\n",
    "    pool_strides=(2,2)\n",
    "    num_outputs_1=256\n",
    "    num_outputs_2=750\n",
    "    conv1=conv2d_maxpool(x_tensor, conv_p2_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    \n",
    "    conv1_dropout=tf.nn.dropout(conv1,keep_prob)\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    conv3=flatten(conv1_dropout)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    conv_full=fully_conn(conv3,num_outputs_1)\n",
    "    conv_full_dropout=tf.nn.dropout(conv_full,keep_prob)\n",
    "   \n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return output(conv_full_dropout,conv_num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    valid_acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    \n",
    "    print('Loss: {:>10.4f} Accuracy: {:>.5f} Accuracy:{}'.format(loss,valid_acc,valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2873 Accuracy: 0.14020 Accuracy:0.14019998908042908\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2594 Accuracy: 0.16240 Accuracy:0.1623999923467636\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2417 Accuracy: 0.19100 Accuracy:0.19099996984004974\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.2172 Accuracy: 0.20420 Accuracy:0.20419998466968536\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1836 Accuracy: 0.23280 Accuracy:0.23279999196529388\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.1340 Accuracy: 0.23880 Accuracy:0.23879998922348022\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.1227 Accuracy: 0.25280 Accuracy:0.25279998779296875\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.0800 Accuracy: 0.26920 Accuracy:0.26919999718666077\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.0685 Accuracy: 0.28340 Accuracy:0.2833999991416931\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.0603 Accuracy: 0.27060 Accuracy:0.27060002088546753\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.0463 Accuracy: 0.28680 Accuracy:0.28679999709129333\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.0076 Accuracy: 0.30100 Accuracy:0.3009999990463257\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.9827 Accuracy: 0.29200 Accuracy:0.2919999957084656\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.9665 Accuracy: 0.31160 Accuracy:0.311599999666214\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.9657 Accuracy: 0.31180 Accuracy:0.3118000030517578\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.9294 Accuracy: 0.30700 Accuracy:0.30699995160102844\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.9197 Accuracy: 0.31180 Accuracy:0.3118000030517578\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.8704 Accuracy: 0.33940 Accuracy:0.3393999934196472\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.8701 Accuracy: 0.34400 Accuracy:0.343999981880188\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.8561 Accuracy: 0.34540 Accuracy:0.34539994597435\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.8472 Accuracy: 0.34860 Accuracy:0.34859997034072876\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.7980 Accuracy: 0.35600 Accuracy:0.35599997639656067\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.8321 Accuracy: 0.35800 Accuracy:0.3579999804496765\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.7696 Accuracy: 0.37360 Accuracy:0.37359997630119324\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.7719 Accuracy: 0.38360 Accuracy:0.38359999656677246\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.7740 Accuracy: 0.39200 Accuracy:0.3919999897480011\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.7590 Accuracy: 0.38480 Accuracy:0.384799987077713\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.7539 Accuracy: 0.38380 Accuracy:0.3837999701499939\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.7451 Accuracy: 0.39680 Accuracy:0.3967999815940857\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.7524 Accuracy: 0.39540 Accuracy:0.3953999876976013\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.6744 Accuracy: 0.39960 Accuracy:0.39959996938705444\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.6892 Accuracy: 0.41820 Accuracy:0.4182000160217285\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.6526 Accuracy: 0.41300 Accuracy:0.4129999876022339\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.5952 Accuracy: 0.42540 Accuracy:0.4253999888896942\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.5961 Accuracy: 0.41780 Accuracy:0.4177999794483185\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.5782 Accuracy: 0.43380 Accuracy:0.43379995226860046\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.5628 Accuracy: 0.43060 Accuracy:0.43059998750686646\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.5187 Accuracy: 0.43760 Accuracy:0.43759995698928833\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.5016 Accuracy: 0.43080 Accuracy:0.43080002069473267\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.4298 Accuracy: 0.44720 Accuracy:0.4471999406814575\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.4419 Accuracy: 0.45840 Accuracy:0.4583999812602997\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.4204 Accuracy: 0.45420 Accuracy:0.45419996976852417\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.3569 Accuracy: 0.46580 Accuracy:0.465800017118454\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.3154 Accuracy: 0.47320 Accuracy:0.4731999337673187\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.2617 Accuracy: 0.47000 Accuracy:0.4699999690055847\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.2718 Accuracy: 0.47400 Accuracy:0.473999947309494\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.2214 Accuracy: 0.48440 Accuracy:0.4843999445438385\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.2060 Accuracy: 0.48780 Accuracy:0.4877999424934387\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.1488 Accuracy: 0.49460 Accuracy:0.49459993839263916\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.0970 Accuracy: 0.49420 Accuracy:0.4941999614238739\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.0759 Accuracy: 0.50240 Accuracy:0.5023999810218811\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.0869 Accuracy: 0.49280 Accuracy:0.4927999675273895\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.0643 Accuracy: 0.50700 Accuracy:0.5069999098777771\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.0523 Accuracy: 0.50900 Accuracy:0.5089999437332153\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.0222 Accuracy: 0.51040 Accuracy:0.5103999376296997\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.0052 Accuracy: 0.50340 Accuracy:0.5033999681472778\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.9684 Accuracy: 0.51420 Accuracy:0.51419997215271\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.9439 Accuracy: 0.50940 Accuracy:0.5093998908996582\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.9207 Accuracy: 0.51460 Accuracy:0.5145999789237976\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.9279 Accuracy: 0.52100 Accuracy:0.5209999084472656\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.9129 Accuracy: 0.52480 Accuracy:0.5247999429702759\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.8694 Accuracy: 0.52020 Accuracy:0.5201999545097351\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.8868 Accuracy: 0.52520 Accuracy:0.5251999497413635\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.8639 Accuracy: 0.52560 Accuracy:0.5255999565124512\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.8621 Accuracy: 0.51960 Accuracy:0.519599974155426\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.8501 Accuracy: 0.52600 Accuracy:0.5259999632835388\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.8128 Accuracy: 0.52860 Accuracy:0.5285999178886414\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.7828 Accuracy: 0.53580 Accuracy:0.5357999801635742\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.7843 Accuracy: 0.53500 Accuracy:0.5349999666213989\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.7954 Accuracy: 0.53500 Accuracy:0.5349999070167542\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.7500 Accuracy: 0.53440 Accuracy:0.5343999266624451\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.7426 Accuracy: 0.53580 Accuracy:0.5357999205589294\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.7672 Accuracy: 0.53760 Accuracy:0.5375999212265015\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.7258 Accuracy: 0.54320 Accuracy:0.5431999564170837\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.7477 Accuracy: 0.53820 Accuracy:0.5381999611854553\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.7165 Accuracy: 0.53780 Accuracy:0.5377999544143677\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.6649 Accuracy: 0.54520 Accuracy:0.5451999306678772\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.6864 Accuracy: 0.54200 Accuracy:0.5419999957084656\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.6479 Accuracy: 0.54000 Accuracy:0.5399999618530273\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.6605 Accuracy: 0.54740 Accuracy:0.5473999381065369\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.6407 Accuracy: 0.54180 Accuracy:0.5417999625205994\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.5986 Accuracy: 0.54860 Accuracy:0.5485999584197998\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.6254 Accuracy: 0.55040 Accuracy:0.550399899482727\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.5659 Accuracy: 0.54900 Accuracy:0.5489999055862427\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.5594 Accuracy: 0.54920 Accuracy:0.5491999387741089\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.5292 Accuracy: 0.55120 Accuracy:0.5511999726295471\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.5143 Accuracy: 0.54760 Accuracy:0.5475999712944031\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.5024 Accuracy: 0.55420 Accuracy:0.5541999340057373\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.5328 Accuracy: 0.54580 Accuracy:0.5457999110221863\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.5146 Accuracy: 0.54740 Accuracy:0.5473998785018921\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.4935 Accuracy: 0.55080 Accuracy:0.5507999062538147\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.4578 Accuracy: 0.54420 Accuracy:0.5441999435424805\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.4607 Accuracy: 0.55120 Accuracy:0.5511999130249023\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.4652 Accuracy: 0.54560 Accuracy:0.5455999374389648\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.4417 Accuracy: 0.55840 Accuracy:0.5583999156951904\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.4165 Accuracy: 0.55800 Accuracy:0.5579999685287476\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.4071 Accuracy: 0.55960 Accuracy:0.5595998764038086\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.4085 Accuracy: 0.55920 Accuracy:0.5591999292373657\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.4160 Accuracy: 0.55460 Accuracy:0.554599940776825\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.3852 Accuracy: 0.55780 Accuracy:0.5577999353408813\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2976 Accuracy: 0.10220 Accuracy:0.10220000147819519\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.3034 Accuracy: 0.09860 Accuracy:0.09860000014305115\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2955 Accuracy: 0.13820 Accuracy:0.13819999992847443\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.2281 Accuracy: 0.15020 Accuracy:0.1501999944448471\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.2093 Accuracy: 0.17880 Accuracy:0.17880000174045563\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1729 Accuracy: 0.17800 Accuracy:0.17799998819828033\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.2050 Accuracy: 0.19280 Accuracy:0.19280000030994415\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.1223 Accuracy: 0.19200 Accuracy:0.19200000166893005\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.0915 Accuracy: 0.19820 Accuracy:0.19820000231266022\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.1206 Accuracy: 0.23200 Accuracy:0.2319999784231186\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1886 Accuracy: 0.24340 Accuracy:0.2433999925851822\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.0651 Accuracy: 0.24780 Accuracy:0.24779997766017914\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.0201 Accuracy: 0.23640 Accuracy:0.23639999330043793\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.0549 Accuracy: 0.23700 Accuracy:0.236999973654747\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.0650 Accuracy: 0.24640 Accuracy:0.24639998376369476\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1442 Accuracy: 0.25520 Accuracy:0.25519996881484985\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.9889 Accuracy: 0.25700 Accuracy:0.25699999928474426\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.9433 Accuracy: 0.25620 Accuracy:0.25619998574256897\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.9385 Accuracy: 0.26580 Accuracy:0.26579999923706055\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.9327 Accuracy: 0.27320 Accuracy:0.27320000529289246\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.0907 Accuracy: 0.28160 Accuracy:0.2815999686717987\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.9514 Accuracy: 0.28620 Accuracy:0.28619998693466187\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.8929 Accuracy: 0.27020 Accuracy:0.2702000141143799\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.8798 Accuracy: 0.27640 Accuracy:0.27639999985694885\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.9068 Accuracy: 0.28180 Accuracy:0.2818000316619873\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.0741 Accuracy: 0.29900 Accuracy:0.29899999499320984\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.9258 Accuracy: 0.27580 Accuracy:0.27580001950263977\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.8207 Accuracy: 0.29820 Accuracy:0.29819998145103455\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.8245 Accuracy: 0.33400 Accuracy:0.33399996161460876\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.9044 Accuracy: 0.32000 Accuracy:0.3199999928474426\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.9817 Accuracy: 0.31940 Accuracy:0.31939995288848877\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.8354 Accuracy: 0.32100 Accuracy:0.32099995017051697\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.7806 Accuracy: 0.33540 Accuracy:0.33539995551109314\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.7706 Accuracy: 0.32580 Accuracy:0.32580000162124634\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.8147 Accuracy: 0.33160 Accuracy:0.33160001039505005\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.9257 Accuracy: 0.33200 Accuracy:0.3319999575614929\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.7987 Accuracy: 0.35360 Accuracy:0.35359999537467957\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.7067 Accuracy: 0.34440 Accuracy:0.34439998865127563\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.7052 Accuracy: 0.36260 Accuracy:0.3625999987125397\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.7569 Accuracy: 0.35680 Accuracy:0.3567999601364136\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.8664 Accuracy: 0.38360 Accuracy:0.38359999656677246\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.6723 Accuracy: 0.38960 Accuracy:0.38960000872612\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.6574 Accuracy: 0.38140 Accuracy:0.3813999891281128\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.5588 Accuracy: 0.40960 Accuracy:0.40959998965263367\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.6599 Accuracy: 0.41900 Accuracy:0.41899996995925903\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.7675 Accuracy: 0.42260 Accuracy:0.4225999712944031\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.5585 Accuracy: 0.42920 Accuracy:0.4291999638080597\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.5538 Accuracy: 0.42600 Accuracy:0.4259999692440033\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.4842 Accuracy: 0.44040 Accuracy:0.44039997458457947\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.5708 Accuracy: 0.44300 Accuracy:0.4429999589920044\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.7414 Accuracy: 0.44840 Accuracy:0.44839999079704285\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.4146 Accuracy: 0.44740 Accuracy:0.44739997386932373\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.4134 Accuracy: 0.45580 Accuracy:0.45579996705055237\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.4387 Accuracy: 0.47600 Accuracy:0.47599995136260986\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.5002 Accuracy: 0.47860 Accuracy:0.4785999357700348\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.6763 Accuracy: 0.47400 Accuracy:0.47399991750717163\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.3301 Accuracy: 0.47460 Accuracy:0.47460001707077026\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.3559 Accuracy: 0.48440 Accuracy:0.4843999743461609\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.3756 Accuracy: 0.49520 Accuracy:0.49519994854927063\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.4973 Accuracy: 0.49760 Accuracy:0.4975999593734741\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5747 Accuracy: 0.50520 Accuracy:0.5051999092102051\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.2816 Accuracy: 0.49580 Accuracy:0.4957999587059021\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.3200 Accuracy: 0.49620 Accuracy:0.49619996547698975\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.2860 Accuracy: 0.51960 Accuracy:0.5195999145507812\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.4081 Accuracy: 0.50220 Accuracy:0.5021999478340149\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.3898 Accuracy: 0.52480 Accuracy:0.5247999429702759\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.1730 Accuracy: 0.51720 Accuracy:0.5171999335289001\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.2529 Accuracy: 0.50500 Accuracy:0.5049998760223389\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.2318 Accuracy: 0.52540 Accuracy:0.525399923324585\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.3471 Accuracy: 0.53040 Accuracy:0.5303999185562134\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.3876 Accuracy: 0.53440 Accuracy:0.5343998670578003\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.1177 Accuracy: 0.52220 Accuracy:0.5221999287605286\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.2197 Accuracy: 0.52380 Accuracy:0.5237998962402344\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.2323 Accuracy: 0.54060 Accuracy:0.5405999422073364\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.2834 Accuracy: 0.54280 Accuracy:0.5427999496459961\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.3373 Accuracy: 0.54640 Accuracy:0.5463999509811401\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.0713 Accuracy: 0.52880 Accuracy:0.5287999510765076\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.1544 Accuracy: 0.54780 Accuracy:0.5477998852729797\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.1966 Accuracy: 0.55180 Accuracy:0.5517998933792114\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.1603 Accuracy: 0.55600 Accuracy:0.5559999346733093\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.2681 Accuracy: 0.55540 Accuracy:0.5553999543190002\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.0188 Accuracy: 0.54800 Accuracy:0.547999918460846\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.1229 Accuracy: 0.55060 Accuracy:0.5505999326705933\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.1408 Accuracy: 0.55980 Accuracy:0.5597999691963196\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.1074 Accuracy: 0.55720 Accuracy:0.5571999549865723\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.2279 Accuracy: 0.55940 Accuracy:0.5593999624252319\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.9369 Accuracy: 0.55820 Accuracy:0.558199942111969\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.0729 Accuracy: 0.55680 Accuracy:0.5567999482154846\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.1084 Accuracy: 0.57300 Accuracy:0.5729999542236328\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.0899 Accuracy: 0.56380 Accuracy:0.5637999176979065\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.2114 Accuracy: 0.56940 Accuracy:0.5693999528884888\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.8781 Accuracy: 0.56760 Accuracy:0.5675999522209167\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.0146 Accuracy: 0.56740 Accuracy:0.5673999190330505\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.0788 Accuracy: 0.57100 Accuracy:0.5709999203681946\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.0286 Accuracy: 0.56860 Accuracy:0.5685999393463135\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.1212 Accuracy: 0.57100 Accuracy:0.5709999203681946\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.8655 Accuracy: 0.57780 Accuracy:0.577799916267395\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.9692 Accuracy: 0.57480 Accuracy:0.5747998952865601\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.0189 Accuracy: 0.57420 Accuracy:0.574199914932251\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.9723 Accuracy: 0.57840 Accuracy:0.5783999562263489\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.1615 Accuracy: 0.57400 Accuracy:0.5739998817443848\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.8044 Accuracy: 0.57240 Accuracy:0.5723999738693237\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.9099 Accuracy: 0.57120 Accuracy:0.5712000131607056\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.9956 Accuracy: 0.57940 Accuracy:0.5793999433517456\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.9453 Accuracy: 0.58260 Accuracy:0.5825998783111572\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.0911 Accuracy: 0.57700 Accuracy:0.5769999027252197\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.8080 Accuracy: 0.58380 Accuracy:0.5837998986244202\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.8891 Accuracy: 0.58120 Accuracy:0.5811999440193176\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.9350 Accuracy: 0.58920 Accuracy:0.5891999006271362\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.9503 Accuracy: 0.57740 Accuracy:0.5773999691009521\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.0822 Accuracy: 0.58420 Accuracy:0.5841999650001526\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.7280 Accuracy: 0.58760 Accuracy:0.5875999331474304\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.7778 Accuracy: 0.58700 Accuracy:0.5869998931884766\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.9125 Accuracy: 0.59820 Accuracy:0.5981999039649963\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.8750 Accuracy: 0.58700 Accuracy:0.5869998931884766\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.0725 Accuracy: 0.59400 Accuracy:0.5939999222755432\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.7397 Accuracy: 0.58640 Accuracy:0.5863999128341675\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.8306 Accuracy: 0.58800 Accuracy:0.5879999399185181\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.8659 Accuracy: 0.60100 Accuracy:0.6009998917579651\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.8922 Accuracy: 0.59300 Accuracy:0.5929999351501465\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.0381 Accuracy: 0.59160 Accuracy:0.5915999412536621\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.7263 Accuracy: 0.59320 Accuracy:0.5931999683380127\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.8366 Accuracy: 0.59320 Accuracy:0.5931999087333679\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.8216 Accuracy: 0.59820 Accuracy:0.5981999039649963\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.8423 Accuracy: 0.59520 Accuracy:0.5951999425888062\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.9833 Accuracy: 0.59020 Accuracy:0.5901999473571777\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.6529 Accuracy: 0.59340 Accuracy:0.5933998823165894\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.7962 Accuracy: 0.59420 Accuracy:0.5941999554634094\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.7989 Accuracy: 0.60580 Accuracy:0.6057999134063721\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.8338 Accuracy: 0.59620 Accuracy:0.5961998701095581\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.9257 Accuracy: 0.60200 Accuracy:0.6019998788833618\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.6829 Accuracy: 0.59920 Accuracy:0.5991999506950378\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.7732 Accuracy: 0.59680 Accuracy:0.596799910068512\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.7269 Accuracy: 0.60360 Accuracy:0.6035999059677124\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.7813 Accuracy: 0.59860 Accuracy:0.598599910736084\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.8900 Accuracy: 0.60380 Accuracy:0.6037999391555786\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.6517 Accuracy: 0.60560 Accuracy:0.6055998802185059\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.7648 Accuracy: 0.60260 Accuracy:0.6025999188423157\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.7130 Accuracy: 0.61260 Accuracy:0.6125999093055725\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.7839 Accuracy: 0.60080 Accuracy:0.6007999777793884\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.9191 Accuracy: 0.60180 Accuracy:0.6017999053001404\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.6371 Accuracy: 0.60240 Accuracy:0.6023999452590942\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.7284 Accuracy: 0.60080 Accuracy:0.6007999181747437\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.6586 Accuracy: 0.60920 Accuracy:0.6091999411582947\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.7321 Accuracy: 0.60200 Accuracy:0.6019999384880066\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.8863 Accuracy: 0.60540 Accuracy:0.6053999066352844\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.6092 Accuracy: 0.60300 Accuracy:0.6029999256134033\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.6793 Accuracy: 0.60380 Accuracy:0.6037998795509338\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.6824 Accuracy: 0.61600 Accuracy:0.6159999370574951\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.7478 Accuracy: 0.60580 Accuracy:0.6057999134063721\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.8881 Accuracy: 0.61040 Accuracy:0.6103999018669128\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.5943 Accuracy: 0.60460 Accuracy:0.6045998930931091\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.6959 Accuracy: 0.60880 Accuracy:0.608799934387207\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.6245 Accuracy: 0.60880 Accuracy:0.6087998747825623\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.6816 Accuracy: 0.60520 Accuracy:0.6051998734474182\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.8471 Accuracy: 0.61340 Accuracy:0.6133999228477478\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.5668 Accuracy: 0.60220 Accuracy:0.602199912071228\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.6832 Accuracy: 0.60800 Accuracy:0.607999861240387\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.6269 Accuracy: 0.61320 Accuracy:0.6131999492645264\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.6910 Accuracy: 0.60920 Accuracy:0.6091998815536499\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.8156 Accuracy: 0.61260 Accuracy:0.6125999093055725\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.5376 Accuracy: 0.61160 Accuracy:0.6115999221801758\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.6402 Accuracy: 0.61820 Accuracy:0.61819988489151\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.5973 Accuracy: 0.61040 Accuracy:0.6103999018669128\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.6624 Accuracy: 0.61380 Accuracy:0.6137999296188354\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.7863 Accuracy: 0.61280 Accuracy:0.612799882888794\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.5219 Accuracy: 0.61220 Accuracy:0.6121999025344849\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.6372 Accuracy: 0.60860 Accuracy:0.608599841594696\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.6202 Accuracy: 0.61600 Accuracy:0.6159998774528503\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.6954 Accuracy: 0.61040 Accuracy:0.6103999018669128\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.7735 Accuracy: 0.61060 Accuracy:0.6105998754501343\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.5584 Accuracy: 0.60880 Accuracy:0.6087998747825623\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.5572 Accuracy: 0.61600 Accuracy:0.6159999370574951\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.5893 Accuracy: 0.62280 Accuracy:0.6227999329566956\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.6503 Accuracy: 0.61080 Accuracy:0.6107999086380005\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.7475 Accuracy: 0.61360 Accuracy:0.6135998964309692\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.5106 Accuracy: 0.61440 Accuracy:0.6143999099731445\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.5969 Accuracy: 0.61720 Accuracy:0.6171998977661133\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.5831 Accuracy: 0.61640 Accuracy:0.616399884223938\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.6183 Accuracy: 0.61320 Accuracy:0.6131998896598816\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.7322 Accuracy: 0.61540 Accuracy:0.6153998374938965\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.4919 Accuracy: 0.61760 Accuracy:0.6175999045372009\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.5508 Accuracy: 0.61940 Accuracy:0.6193998456001282\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.5513 Accuracy: 0.62680 Accuracy:0.6267998814582825\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.6482 Accuracy: 0.61620 Accuracy:0.6161999106407166\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.6980 Accuracy: 0.61920 Accuracy:0.6191999316215515\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.4812 Accuracy: 0.61880 Accuracy:0.6187999248504639\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.5617 Accuracy: 0.62060 Accuracy:0.6205999255180359\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.5388 Accuracy: 0.62280 Accuracy:0.6227999329566956\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.6254 Accuracy: 0.61680 Accuracy:0.6167999505996704\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.6725 Accuracy: 0.61860 Accuracy:0.6185998916625977\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.4738 Accuracy: 0.61980 Accuracy:0.6197999119758606\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.5463 Accuracy: 0.61760 Accuracy:0.6175999045372009\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.5178 Accuracy: 0.62280 Accuracy:0.6227998733520508\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.5605 Accuracy: 0.62360 Accuracy:0.6235998868942261\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.6548 Accuracy: 0.62040 Accuracy:0.6203998923301697\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.4433 Accuracy: 0.62080 Accuracy:0.6207998991012573\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.5273 Accuracy: 0.62240 Accuracy:0.6223998665809631\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.5469 Accuracy: 0.62940 Accuracy:0.6293998956680298\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.5906 Accuracy: 0.62180 Accuracy:0.6217999458312988\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.6773 Accuracy: 0.61700 Accuracy:0.6169998645782471\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.4605 Accuracy: 0.61520 Accuracy:0.615199863910675\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.5366 Accuracy: 0.62360 Accuracy:0.6235998868942261\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.5200 Accuracy: 0.62500 Accuracy:0.6249999403953552\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.5398 Accuracy: 0.62540 Accuracy:0.6253998875617981\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.6625 Accuracy: 0.61800 Accuracy:0.6179998517036438\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.4673 Accuracy: 0.61400 Accuracy:0.6139999032020569\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.5752 Accuracy: 0.61940 Accuracy:0.619399905204773\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.4908 Accuracy: 0.63020 Accuracy:0.6301999092102051\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.5288 Accuracy: 0.62480 Accuracy:0.624799907207489\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.6330 Accuracy: 0.62680 Accuracy:0.6267998218536377\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.4512 Accuracy: 0.61620 Accuracy:0.6161999106407166\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.5186 Accuracy: 0.62020 Accuracy:0.6201999187469482\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.5122 Accuracy: 0.63140 Accuracy:0.631399929523468\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.5494 Accuracy: 0.62360 Accuracy:0.6235998868942261\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.6343 Accuracy: 0.63420 Accuracy:0.6341999173164368\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.4018 Accuracy: 0.62680 Accuracy:0.6267999410629272\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.5057 Accuracy: 0.62180 Accuracy:0.621799886226654\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.4668 Accuracy: 0.62700 Accuracy:0.6269999742507935\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.5243 Accuracy: 0.62380 Accuracy:0.6237999200820923\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.6452 Accuracy: 0.62080 Accuracy:0.6207998991012573\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.4116 Accuracy: 0.62200 Accuracy:0.6219998598098755\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.5427 Accuracy: 0.62140 Accuracy:0.6213999390602112\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.4331 Accuracy: 0.62880 Accuracy:0.6287998557090759\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.5271 Accuracy: 0.62360 Accuracy:0.6235998868942261\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.6076 Accuracy: 0.62440 Accuracy:0.6243999004364014\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.4426 Accuracy: 0.62620 Accuracy:0.6261999011039734\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.4803 Accuracy: 0.62460 Accuracy:0.6245999336242676\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.4469 Accuracy: 0.63180 Accuracy:0.6317998766899109\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.4859 Accuracy: 0.62500 Accuracy:0.6249999403953552\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.6092 Accuracy: 0.62400 Accuracy:0.623999834060669\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.4107 Accuracy: 0.62720 Accuracy:0.6271998882293701\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.4798 Accuracy: 0.62100 Accuracy:0.6209999322891235\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.4295 Accuracy: 0.62700 Accuracy:0.6269999146461487\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.4871 Accuracy: 0.62940 Accuracy:0.629399836063385\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.5776 Accuracy: 0.63040 Accuracy:0.6303998827934265\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.4031 Accuracy: 0.62460 Accuracy:0.6245998740196228\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.4440 Accuracy: 0.63160 Accuracy:0.6315999031066895\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.4087 Accuracy: 0.63340 Accuracy:0.6333998441696167\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.4753 Accuracy: 0.62660 Accuracy:0.626599907875061\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.6039 Accuracy: 0.62700 Accuracy:0.6269999146461487\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.4007 Accuracy: 0.63120 Accuracy:0.6311998963356018\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.4828 Accuracy: 0.62260 Accuracy:0.6225998997688293\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.4164 Accuracy: 0.63740 Accuracy:0.6373999118804932\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.4505 Accuracy: 0.63260 Accuracy:0.6325998902320862\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.5674 Accuracy: 0.63120 Accuracy:0.6311998963356018\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.3873 Accuracy: 0.61760 Accuracy:0.6175998449325562\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.4566 Accuracy: 0.62200 Accuracy:0.6219999194145203\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.4440 Accuracy: 0.62820 Accuracy:0.6281998753547668\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.4835 Accuracy: 0.63040 Accuracy:0.6303999423980713\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.5300 Accuracy: 0.63180 Accuracy:0.6317999362945557\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.3918 Accuracy: 0.62500 Accuracy:0.6249999403953552\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.4265 Accuracy: 0.63280 Accuracy:0.6327998638153076\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.4155 Accuracy: 0.64000 Accuracy:0.6399998664855957\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.4300 Accuracy: 0.63640 Accuracy:0.6363998651504517\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.5389 Accuracy: 0.63220 Accuracy:0.6321998834609985\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.3662 Accuracy: 0.62620 Accuracy:0.6261999607086182\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.4372 Accuracy: 0.63280 Accuracy:0.6327999234199524\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.4022 Accuracy: 0.62860 Accuracy:0.6285998821258545\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.5091 Accuracy: 0.62880 Accuracy:0.6287999153137207\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.4948 Accuracy: 0.63600 Accuracy:0.6359999179840088\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.3659 Accuracy: 0.63220 Accuracy:0.6321998834609985\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.4185 Accuracy: 0.63620 Accuracy:0.6361998915672302\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.4175 Accuracy: 0.63660 Accuracy:0.6365998983383179\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.4474 Accuracy: 0.63360 Accuracy:0.6335998773574829\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.5409 Accuracy: 0.63100 Accuracy:0.6309998631477356\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.3827 Accuracy: 0.63120 Accuracy:0.6311998963356018\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.4756 Accuracy: 0.63080 Accuracy:0.6307998895645142\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.4436 Accuracy: 0.63500 Accuracy:0.6349998712539673\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.4650 Accuracy: 0.63020 Accuracy:0.6301999092102051\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.5507 Accuracy: 0.63120 Accuracy:0.6311998963356018\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.3391 Accuracy: 0.63280 Accuracy:0.6327998638153076\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.4072 Accuracy: 0.63480 Accuracy:0.6347998976707458\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.4257 Accuracy: 0.63300 Accuracy:0.6329999566078186\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.4804 Accuracy: 0.63560 Accuracy:0.6355999112129211\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.5031 Accuracy: 0.63300 Accuracy:0.6329998970031738\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.3702 Accuracy: 0.62600 Accuracy:0.6259998679161072\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.3946 Accuracy: 0.62960 Accuracy:0.6295998692512512\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.3861 Accuracy: 0.63060 Accuracy:0.6305999159812927\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.3984 Accuracy: 0.63500 Accuracy:0.6349999308586121\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.5194 Accuracy: 0.63500 Accuracy:0.6349998712539673\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.3669 Accuracy: 0.63240 Accuracy:0.6323999166488647\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.3551 Accuracy: 0.63300 Accuracy:0.6329998970031738\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.3683 Accuracy: 0.63660 Accuracy:0.6365998983383179\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.4455 Accuracy: 0.62960 Accuracy:0.6295998692512512\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.5481 Accuracy: 0.63580 Accuracy:0.6357998847961426\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.3274 Accuracy: 0.61940 Accuracy:0.619399905204773\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.3680 Accuracy: 0.63360 Accuracy:0.6335998177528381\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.3865 Accuracy: 0.64080 Accuracy:0.640799880027771\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.4259 Accuracy: 0.63540 Accuracy:0.6353998780250549\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.4998 Accuracy: 0.63280 Accuracy:0.6327999234199524\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.3505 Accuracy: 0.62300 Accuracy:0.622999906539917\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.3746 Accuracy: 0.63460 Accuracy:0.6345999240875244\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.3814 Accuracy: 0.63400 Accuracy:0.6339998841285706\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.3889 Accuracy: 0.63020 Accuracy:0.6301999092102051\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.4694 Accuracy: 0.63240 Accuracy:0.63239985704422\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.3676 Accuracy: 0.62200 Accuracy:0.6219998598098755\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.3854 Accuracy: 0.63360 Accuracy:0.6335999369621277\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.3759 Accuracy: 0.63880 Accuracy:0.6387999057769775\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.3598 Accuracy: 0.63880 Accuracy:0.6387999057769775\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.4954 Accuracy: 0.62940 Accuracy:0.6293999552726746\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.3220 Accuracy: 0.62860 Accuracy:0.6285998821258545\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.3870 Accuracy: 0.62800 Accuracy:0.6279999017715454\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.3992 Accuracy: 0.63640 Accuracy:0.6363998651504517\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.3863 Accuracy: 0.63940 Accuracy:0.6393998861312866\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.4702 Accuracy: 0.63760 Accuracy:0.6375999450683594\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.3499 Accuracy: 0.62280 Accuracy:0.6227999329566956\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.3962 Accuracy: 0.63260 Accuracy:0.6325998306274414\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.3768 Accuracy: 0.63620 Accuracy:0.6361998319625854\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.3403 Accuracy: 0.63620 Accuracy:0.6361998915672302\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.4758 Accuracy: 0.64020 Accuracy:0.6401998400688171\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.3082 Accuracy: 0.63340 Accuracy:0.6333999037742615\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.3980 Accuracy: 0.64080 Accuracy:0.640799880027771\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.3495 Accuracy: 0.64440 Accuracy:0.644399881362915\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.3444 Accuracy: 0.64040 Accuracy:0.6403999328613281\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.4840 Accuracy: 0.63440 Accuracy:0.634399950504303\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.2958 Accuracy: 0.62280 Accuracy:0.6227998733520508\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.3812 Accuracy: 0.63580 Accuracy:0.6357999444007874\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.3710 Accuracy: 0.63700 Accuracy:0.6369999051094055\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.3669 Accuracy: 0.63200 Accuracy:0.6319999098777771\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.4783 Accuracy: 0.63980 Accuracy:0.6397998929023743\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.2910 Accuracy: 0.63300 Accuracy:0.6329998970031738\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.3542 Accuracy: 0.63220 Accuracy:0.6321999430656433\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.3395 Accuracy: 0.63700 Accuracy:0.6369998455047607\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.3153 Accuracy: 0.63580 Accuracy:0.6357998847961426\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.4385 Accuracy: 0.64060 Accuracy:0.6405999064445496\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.3072 Accuracy: 0.62520 Accuracy:0.6251999139785767\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.3656 Accuracy: 0.62940 Accuracy:0.6293998956680298\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.3449 Accuracy: 0.63540 Accuracy:0.6353998780250549\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.3221 Accuracy: 0.63280 Accuracy:0.6327999234199524\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.4649 Accuracy: 0.63220 Accuracy:0.6321998834609985\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.3069 Accuracy: 0.63280 Accuracy:0.6327998638153076\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.3860 Accuracy: 0.62860 Accuracy:0.6285999417304993\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.3092 Accuracy: 0.64160 Accuracy:0.6415998935699463\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.3588 Accuracy: 0.63760 Accuracy:0.6375998854637146\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.4751 Accuracy: 0.63760 Accuracy:0.6375998854637146\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.2611 Accuracy: 0.63360 Accuracy:0.6335999369621277\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.3110 Accuracy: 0.63320 Accuracy:0.63319993019104\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.3295 Accuracy: 0.63720 Accuracy:0.637199878692627\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.3682 Accuracy: 0.63460 Accuracy:0.6345998644828796\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.4512 Accuracy: 0.63960 Accuracy:0.6395998597145081\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.2802 Accuracy: 0.63120 Accuracy:0.6311998963356018\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.3495 Accuracy: 0.63600 Accuracy:0.635999858379364\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.3797 Accuracy: 0.63400 Accuracy:0.6339999437332153\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.3576 Accuracy: 0.63580 Accuracy:0.6357998847961426\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.4272 Accuracy: 0.63020 Accuracy:0.6301999092102051\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.2853 Accuracy: 0.63240 Accuracy:0.6323999166488647\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.3254 Accuracy: 0.62660 Accuracy:0.626599907875061\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.3167 Accuracy: 0.63720 Accuracy:0.637199878692627\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.3452 Accuracy: 0.63300 Accuracy:0.6329999566078186\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.4133 Accuracy: 0.63780 Accuracy:0.637799859046936\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.2879 Accuracy: 0.63740 Accuracy:0.6373999118804932\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.3305 Accuracy: 0.63840 Accuracy:0.6383998990058899\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.3107 Accuracy: 0.64040 Accuracy:0.6403999328613281\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.3339 Accuracy: 0.63380 Accuracy:0.6337999105453491\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.4509 Accuracy: 0.63000 Accuracy:0.6299998760223389\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.2963 Accuracy: 0.62780 Accuracy:0.627799928188324\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.3311 Accuracy: 0.62540 Accuracy:0.6253998875617981\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.3077 Accuracy: 0.63440 Accuracy:0.6343998908996582\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.3253 Accuracy: 0.63520 Accuracy:0.6351998448371887\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.4277 Accuracy: 0.63340 Accuracy:0.6333998441696167\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.2808 Accuracy: 0.62940 Accuracy:0.6293998956680298\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.3470 Accuracy: 0.63080 Accuracy:0.6307999491691589\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.3088 Accuracy: 0.64000 Accuracy:0.6399998664855957\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.3121 Accuracy: 0.63440 Accuracy:0.6343998908996582\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.4090 Accuracy: 0.63160 Accuracy:0.6315999627113342\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.2420 Accuracy: 0.62780 Accuracy:0.627799928188324\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.2869 Accuracy: 0.63720 Accuracy:0.637199878692627\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.2599 Accuracy: 0.64300 Accuracy:0.6429999470710754\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.3229 Accuracy: 0.63300 Accuracy:0.632999837398529\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.3985 Accuracy: 0.63900 Accuracy:0.6389999389648438\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.2316 Accuracy: 0.63140 Accuracy:0.6313998699188232\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.3279 Accuracy: 0.63960 Accuracy:0.6395999193191528\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.3085 Accuracy: 0.63580 Accuracy:0.6357998847961426\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.3530 Accuracy: 0.63660 Accuracy:0.6365998983383179\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.3491 Accuracy: 0.64500 Accuracy:0.6449998617172241\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.2402 Accuracy: 0.63800 Accuracy:0.6379998922348022\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.3041 Accuracy: 0.63680 Accuracy:0.6367998719215393\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.2903 Accuracy: 0.64180 Accuracy:0.6417999267578125\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.3392 Accuracy: 0.64140 Accuracy:0.6413998603820801\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.3812 Accuracy: 0.63940 Accuracy:0.6393999457359314\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.2345 Accuracy: 0.63560 Accuracy:0.6355999708175659\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.3040 Accuracy: 0.63400 Accuracy:0.6339998841285706\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.2827 Accuracy: 0.64240 Accuracy:0.6423999071121216\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.2822 Accuracy: 0.64520 Accuracy:0.6451998949050903\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.4066 Accuracy: 0.64440 Accuracy:0.6443999409675598\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.2332 Accuracy: 0.64360 Accuracy:0.6435999274253845\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.2680 Accuracy: 0.63320 Accuracy:0.6331998705863953\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.3073 Accuracy: 0.64100 Accuracy:0.6409999132156372\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.3601 Accuracy: 0.63260 Accuracy:0.632599949836731\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.3746 Accuracy: 0.63920 Accuracy:0.6391998529434204\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.2298 Accuracy: 0.63560 Accuracy:0.6355998516082764\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.3146 Accuracy: 0.63700 Accuracy:0.6369999051094055\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.3075 Accuracy: 0.64240 Accuracy:0.6423999071121216\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.3559 Accuracy: 0.63820 Accuracy:0.6381998658180237\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.3732 Accuracy: 0.64220 Accuracy:0.6421999335289001\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.2396 Accuracy: 0.63680 Accuracy:0.6367999315261841\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.3083 Accuracy: 0.63500 Accuracy:0.6349998712539673\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.3160 Accuracy: 0.64160 Accuracy:0.6415998935699463\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.3048 Accuracy: 0.63660 Accuracy:0.6365999579429626\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.3578 Accuracy: 0.64220 Accuracy:0.6421998739242554\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.2602 Accuracy: 0.61420 Accuracy:0.6141999363899231\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.2846 Accuracy: 0.63260 Accuracy:0.632599949836731\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.2860 Accuracy: 0.63880 Accuracy:0.6387999057769775\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.3235 Accuracy: 0.63640 Accuracy:0.6363999247550964\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.3744 Accuracy: 0.64120 Accuracy:0.6411998867988586\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.2590 Accuracy: 0.63460 Accuracy:0.6345999240875244\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.2792 Accuracy: 0.63220 Accuracy:0.6321998834609985\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.3278 Accuracy: 0.64100 Accuracy:0.6409999132156372\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.3291 Accuracy: 0.63700 Accuracy:0.6369999051094055\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.3728 Accuracy: 0.63520 Accuracy:0.6351999044418335\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.2498 Accuracy: 0.62960 Accuracy:0.6295998692512512\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.2552 Accuracy: 0.64220 Accuracy:0.6421998739242554\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.2945 Accuracy: 0.64100 Accuracy:0.6409999132156372\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.3138 Accuracy: 0.64260 Accuracy:0.642599880695343\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.3307 Accuracy: 0.63560 Accuracy:0.6355999112129211\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.2400 Accuracy: 0.62980 Accuracy:0.6297998428344727\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.2466 Accuracy: 0.63680 Accuracy:0.6367999315261841\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.2973 Accuracy: 0.64760 Accuracy:0.6475998759269714\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.3063 Accuracy: 0.63540 Accuracy:0.6353999376296997\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.3585 Accuracy: 0.63840 Accuracy:0.6383999586105347\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.2302 Accuracy: 0.63620 Accuracy:0.6361998319625854\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.2402 Accuracy: 0.63740 Accuracy:0.6373999118804932\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.2942 Accuracy: 0.63880 Accuracy:0.6387999057769775\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.3444 Accuracy: 0.63060 Accuracy:0.6305999159812927\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.3425 Accuracy: 0.63820 Accuracy:0.6381999254226685\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.2399 Accuracy: 0.63680 Accuracy:0.6367998719215393\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.2681 Accuracy: 0.64540 Accuracy:0.6453998684883118\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.2690 Accuracy: 0.64040 Accuracy:0.6403998732566833\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.2471 Accuracy: 0.64040 Accuracy:0.6403998732566833\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.3534 Accuracy: 0.64760 Accuracy:0.6475998759269714\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.2310 Accuracy: 0.63060 Accuracy:0.6305999159812927\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.2586 Accuracy: 0.63740 Accuracy:0.6373999118804932\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.2830 Accuracy: 0.65000 Accuracy:0.6499999761581421\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.2864 Accuracy: 0.64100 Accuracy:0.6409999132156372\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.3693 Accuracy: 0.63660 Accuracy:0.6365998983383179\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.2050 Accuracy: 0.63360 Accuracy:0.6335998773574829\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.2526 Accuracy: 0.64480 Accuracy:0.6447999477386475\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.2578 Accuracy: 0.64280 Accuracy:0.6427998542785645\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.3655 Accuracy: 0.63620 Accuracy:0.6361998915672302\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.3463 Accuracy: 0.64620 Accuracy:0.6461998820304871\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.2078 Accuracy: 0.64500 Accuracy:0.6449999213218689\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.2799 Accuracy: 0.64220 Accuracy:0.6421998739242554\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.2852 Accuracy: 0.64680 Accuracy:0.6467999219894409\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.3045 Accuracy: 0.63980 Accuracy:0.639799952507019\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.3374 Accuracy: 0.64660 Accuracy:0.6465998888015747\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.2227 Accuracy: 0.63680 Accuracy:0.6367998719215393\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.2609 Accuracy: 0.63900 Accuracy:0.638999879360199\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.2879 Accuracy: 0.64200 Accuracy:0.6419999003410339\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.3247 Accuracy: 0.63680 Accuracy:0.6367998123168945\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.3422 Accuracy: 0.64300 Accuracy:0.6429998874664307\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.2144 Accuracy: 0.64120 Accuracy:0.6411999464035034\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.2668 Accuracy: 0.63840 Accuracy:0.6383998394012451\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.2429 Accuracy: 0.63920 Accuracy:0.6391999125480652\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.2835 Accuracy: 0.64200 Accuracy:0.6419999599456787\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.3223 Accuracy: 0.64340 Accuracy:0.6433998346328735\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.2588 Accuracy: 0.63220 Accuracy:0.6321998834609985\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.2992 Accuracy: 0.63780 Accuracy:0.637799859046936\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.3038 Accuracy: 0.64200 Accuracy:0.6419999003410339\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.2647 Accuracy: 0.64360 Accuracy:0.6435998678207397\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.3464 Accuracy: 0.64760 Accuracy:0.6475998759269714\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.2278 Accuracy: 0.63680 Accuracy:0.6367998719215393\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.2750 Accuracy: 0.64060 Accuracy:0.6405999064445496\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.2620 Accuracy: 0.64820 Accuracy:0.6481998562812805\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.2985 Accuracy: 0.64000 Accuracy:0.6399998664855957\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.3439 Accuracy: 0.64180 Accuracy:0.6417999267578125\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.2244 Accuracy: 0.63900 Accuracy:0.638999879360199\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.2858 Accuracy: 0.64500 Accuracy:0.6449998617172241\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.2521 Accuracy: 0.64580 Accuracy:0.6457998752593994\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.2730 Accuracy: 0.64480 Accuracy:0.6447998881340027\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.2512 Accuracy: 0.64900 Accuracy:0.6489999294281006\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.1933 Accuracy: 0.64560 Accuracy:0.6455999612808228\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.2870 Accuracy: 0.64280 Accuracy:0.6427999138832092\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.2671 Accuracy: 0.65120 Accuracy:0.6511998772621155\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.2514 Accuracy: 0.63960 Accuracy:0.6395999193191528\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.3149 Accuracy: 0.64200 Accuracy:0.6419999599456787\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.2302 Accuracy: 0.62880 Accuracy:0.6287999153137207\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.2643 Accuracy: 0.64180 Accuracy:0.6417999267578125\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.3033 Accuracy: 0.64900 Accuracy:0.6489998698234558\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.2598 Accuracy: 0.64340 Accuracy:0.6433998942375183\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.3072 Accuracy: 0.64780 Accuracy:0.6477998495101929\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.1929 Accuracy: 0.64040 Accuracy:0.6403998732566833\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.2302 Accuracy: 0.64580 Accuracy:0.6457998752593994\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.2419 Accuracy: 0.64580 Accuracy:0.6457998752593994\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.2524 Accuracy: 0.64620 Accuracy:0.6461998820304871\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.3161 Accuracy: 0.64140 Accuracy:0.6413998603820801\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.2037 Accuracy: 0.63880 Accuracy:0.6387998461723328\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.2112 Accuracy: 0.64480 Accuracy:0.6447998881340027\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.2397 Accuracy: 0.64540 Accuracy:0.6453999280929565\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.2963 Accuracy: 0.64420 Accuracy:0.6441999077796936\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.3090 Accuracy: 0.64520 Accuracy:0.6451998353004456\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.2480 Accuracy: 0.61820 Accuracy:0.6181999444961548\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.2330 Accuracy: 0.64280 Accuracy:0.6427998542785645\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.2802 Accuracy: 0.64180 Accuracy:0.6417999267578125\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.2176 Accuracy: 0.64680 Accuracy:0.6467999219894409\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.3237 Accuracy: 0.64760 Accuracy:0.6475998759269714\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.2480 Accuracy: 0.63620 Accuracy:0.6361998915672302\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.2191 Accuracy: 0.64580 Accuracy:0.6457998752593994\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.2938 Accuracy: 0.64620 Accuracy:0.6461998820304871\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.2741 Accuracy: 0.64040 Accuracy:0.6403999328613281\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6503164556962026\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9beeddf0f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
